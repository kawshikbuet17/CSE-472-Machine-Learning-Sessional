{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionLayer:\n",
    " \n",
    "    def __init__(self, num_of_filters, kernel_size, padding, stride=1):\n",
    "        self.num_of_filters = num_of_filters\n",
    "        self.kernel_size_h = kernel_size\n",
    "        self.kernel_size_w = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    " \n",
    "    def forward(self, input_data):\n",
    "        self.input_shape = input_data.shape\n",
    "        self.input_data = input_data\n",
    "        batch_size, height, width, channels = input_data.shape\n",
    " \n",
    "        # print(\"input data shape: \", input_data.shape)\n",
    "        # print(batch_size, height, width, channels)\n",
    "        output_height = int(math.floor(\n",
    "            (height - self.kernel_size_h + 2 * self.padding)) / self.stride + 1)\n",
    "        output_width = int(math.floor(\n",
    "            (width - self.kernel_size_w + 2 * self.padding)) / self.stride + 1)\n",
    "        output_shape = (batch_size, output_height,\n",
    "                        output_width, self.num_of_filters)\n",
    " \n",
    "        if self.weights is None:\n",
    " \n",
    "            # initialize weights and biases\n",
    "            # also include channels in the shape\n",
    "            # do i have to also include the channel numbers when initializing the weights?\n",
    "            self.weights = np.random.randn(self.num_of_filters, self.kernel_size_h,\n",
    "                                           self.kernel_size_w, channels) / np.sqrt(self.kernel_size_h * self.kernel_size_w * channels)\n",
    " \n",
    "            # print(\"weights shape: \", self.weights.shape)\n",
    " \n",
    " \n",
    " \n",
    "        if self.biases is None:\n",
    "            self.biases = np.zeros(self.num_of_filters)\n",
    " \n",
    "        # pad the input data\n",
    "        self.input_data_padded = np.pad(input_data, ((\n",
    "            0, 0), (self.padding, self.padding), (self.padding, self.padding), (0, 0)), 'constant')\n",
    " \n",
    " \n",
    "        output = np.zeros(output_shape)\n",
    " \n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                input_matrix = self.input_data_padded[:, i * self.stride: i * self.stride +\n",
    "                                                      self.kernel_size_h, j * self.stride: j * self.stride + self.kernel_size_w, :]\n",
    "                input_matrix = input_matrix.reshape(batch_size, -1)\n",
    "                # print(\"input matrix: \", input_matrix.shape)\n",
    "                # print(\"weights: \", self.weights.shape)\n",
    "                temp_weight = self.weights.reshape(self.num_of_filters, -1)\n",
    "                # print(\"temp weight: \", temp_weight.shape)\n",
    "                output[:, i, j, :] = np.dot(\n",
    "                    input_matrix, temp_weight.T) + self.biases\n",
    " \n",
    "        return output\n",
    " \n",
    "    def backprop(self, output_error, learning_rate=0.05):\n",
    "        lr = learning_rate\n",
    "        batch_size, height, width, channels = self.input_shape\n",
    "        output_height, output_width, output_channels = output_error.shape[1:]\n",
    " \n",
    "        # print(\"output error shape: \", output_error.shape)\n",
    "        # print(\"input shape: \", self.input_shape)\n",
    " \n",
    "        # initialize the weight error and bias error\n",
    "        weight_error = np.zeros(self.weights.shape)\n",
    "        bias_error = np.zeros(self.biases.shape)\n",
    " \n",
    "        # initialize the input error\n",
    "        input_error = np.zeros(self.input_data.shape)\n",
    " \n",
    "        # initialize the input error for padding\n",
    "        input_error_padded = np.zeros(self.input_data_padded.shape)\n",
    " \n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                input_matrix = self.input_data_padded[:, i * self.stride: i * self.stride +\n",
    "                                                      self.kernel_size_h, j * self.stride: j * self.stride + self.kernel_size_w, :]\n",
    "                input_matrix = input_matrix.reshape(batch_size, -1)\n",
    "                # print(\"input matrix: \", input_matrix.shape)\n",
    "                # print(\"weights: \", self.weights.shape)\n",
    "                temp_weight = self.weights.reshape(self.num_of_filters, -1)\n",
    "                # print(\"temp weight: \", temp_weight.shape)\n",
    "                weight_error += np.dot(output_error[:, i, j, :].T,\n",
    "                                       input_matrix).reshape(self.weights.shape)\n",
    "                bias_error += np.sum(output_error[:, i, j, :], axis=0)\n",
    " \n",
    "                input_error_padded[:, i * self.stride: i * self.stride + self.kernel_size_h, j * self.stride: j * self.stride + self.kernel_size_w, :] += np.dot(\n",
    "                    output_error[:, i, j, :], temp_weight).reshape(batch_size, self.kernel_size_h, self.kernel_size_w, channels)\n",
    " \n",
    "        # remove the padding from the input error\n",
    "        input_error = input_error_padded[:, self.padding: self.padding + height,\n",
    "                                         self.padding: self.padding + width, :]\n",
    " \n",
    "        # update the weights and biases\n",
    "        self.weights = self.update_weight(weight_error, lr)\n",
    "        self.biases = self.update_bias(bias_error, lr)\n",
    " \n",
    "        return input_error\n",
    " \n",
    "    def update_weight(self, weight_error, lr):\n",
    "        temp_weight = self.weights - lr * weight_error\n",
    "        return temp_weight\n",
    " \n",
    "    def update_bias(self, bias_error, lr):\n",
    "        temp_bias = self.biases - lr * bias_error\n",
    "        return temp_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer:\n",
    "    def __init__(self):\n",
    "        self.last_input = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def backprop(self, d_L_d_out, learning_rate):\n",
    "        d_L_d_input = d_L_d_out.copy()\n",
    "        d_L_d_input[self.last_input <= 0] = 0\n",
    "        return d_L_d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1160,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPoolingLayer:\n",
    " \n",
    "    def __init__(self, pool_size , stride=2):\n",
    "        pool_size = (pool_size, pool_size)\n",
    "        self.pool_size = pool_size\n",
    " \n",
    "        self.stride = stride\n",
    " \n",
    "    def forward(self, input_data):\n",
    " \n",
    "        self.input_shape = input_data.shape\n",
    " \n",
    "        self.input_data = input_data\n",
    " \n",
    "        batch_size, height, width, channels = input_data.shape\n",
    " \n",
    " \n",
    " \n",
    "        output_height = int((height - self.pool_size[0]) / self.stride + 1)\n",
    " \n",
    "        output_width = int((width - self.pool_size[1]) / self.stride + 1)\n",
    " \n",
    "        output_shape = (batch_size, output_height, output_width, channels)\n",
    " \n",
    "        output = np.zeros(output_shape)\n",
    " \n",
    "        for b in range(batch_size):\n",
    " \n",
    "            h_indices = np.arange(\n",
    "                0, height - self.pool_size[0] + 1, self.stride)\n",
    " \n",
    "            w_indices = np.arange(\n",
    "                0, width - self.pool_size[1] + 1, self.stride)\n",
    " \n",
    "            for i in h_indices:\n",
    " \n",
    "                for j in w_indices:\n",
    " \n",
    " \n",
    "                    current_region = input_data[b, i:i +\n",
    "                                                self.pool_size[0], j:j+self.pool_size[1], :]\n",
    " \n",
    "                    output[b, int(i/self.stride), int(j/self.stride),\n",
    "                           :] = np.max(current_region, axis=(0, 1))\n",
    " \n",
    " \n",
    "        return output\n",
    " \n",
    " \n",
    " \n",
    "    def backprop(self, output_error, learning_rate):\n",
    " \n",
    "        batch_size, height, width, channels = output_error.shape\n",
    " \n",
    "    # initialize the gradient of the input data\n",
    " \n",
    "        input_data_gradient = np.zeros(self.input_shape)\n",
    " \n",
    "        stride = self.stride\n",
    "        pool_size = self.pool_size\n",
    " \n",
    "        for b in range(batch_size):\n",
    " \n",
    "            for m in range(channels):\n",
    " \n",
    "                for i in range(0, height - pool_size[0] + 1, stride):\n",
    " \n",
    "                    for j in range(0, width - pool_size[1] + 1, stride):\n",
    " \n",
    "                        current_region = self.input_data[b, i:i + pool_size[0], j:j+pool_size[1], m]\n",
    " \n",
    "                        max_value = np.max(current_region)\n",
    "                        max_index = np.argmax(current_region)\n",
    " \n",
    "                        k, l = np.unravel_index(max_index, pool_size)\n",
    " \n",
    "                        input_data_gradient[b, i+k, j+l, m] = output_error[b, int(i/stride), int(j/stride), m]\n",
    " \n",
    " \n",
    "        return input_data_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten Layer\n",
    "class FlattenLayer:\n",
    "    def __init__(self):\n",
    "        self.last_input_shape = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.last_input_shape = input.shape\n",
    "        return input.reshape(input.shape[0], -1)\n",
    "    \n",
    "    def backprop(self, grad_output, learning_rate):\n",
    "        return grad_output.reshape(self.last_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully Connected Layer\n",
    "class FullyConnectedLayer:\n",
    "    def __init__(self, output_size):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.output_size = output_size\n",
    "        self.input = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # initialize weights and bias\n",
    "        if self.weights is None:\n",
    "            self.weights = np.random.randn(input.shape[1], self.output_size)\n",
    "        if self.bias is None:\n",
    "            self.bias = np.random.randn(self.output_size)\n",
    "\n",
    "        self.input = input\n",
    "\n",
    "        return np.dot(input, self.weights) + self.bias\n",
    "\n",
    "    def backprop(self, grad_output, learning_rate):\n",
    "        grad_input = np.dot(grad_output, self.weights.T)\n",
    "        grad_weights = np.dot(self.input.T, grad_output)\n",
    "        grad_bias = np.sum(grad_output, axis=0)\n",
    "\n",
    "        self.weights -= learning_rate * grad_weights\n",
    "        self.bias -= learning_rate * grad_bias\n",
    "\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax Layer\n",
    "class SoftmaxLayer:\n",
    "    def __init__(self):\n",
    "        self.last_input = None\n",
    "    \n",
    "    def forward(self, input):\n",
    "        self.last_input = input\n",
    "        exp = np.exp(input - np.max(input, axis=1, keepdims=True))\n",
    "        return exp / np.sum(exp, axis=1, keepdims=True)\n",
    "    \n",
    "    def backprop(self, grad_output, learning_rate):\n",
    "        return grad_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images from folders\n",
    "def load_data_from_folder(image_folder, label_path):\n",
    "    images = []\n",
    "    for filename in os.listdir(image_folder):\n",
    "        img = cv2.imread(os.path.join(image_folder, filename))\n",
    "        if img is not None:\n",
    "            \n",
    "\n",
    "            # convert to grayscale\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # resize to 28x28x1\n",
    "            img = cv2.resize(img, (28, 28), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "            # convert to float32\n",
    "            img = img.astype(np.float32)\n",
    "\n",
    "            images.append(img)\n",
    "\n",
    "            if len(images) == 700:\n",
    "                break\n",
    "\n",
    "    df = pd.read_csv(label_path)\n",
    "    labels = df['digit'].values\n",
    "    labels = labels[:len(images)]\n",
    "\n",
    "\n",
    "    # rotate images iwth 90, 180, 270 degrees and coreesponding labels\n",
    "    for i in range(len(images)):\n",
    "        images.append(np.rot90(images[i], 1))\n",
    "        labels = np.append(labels, labels[i])\n",
    "        images.append(np.rot90(images[i], 1))\n",
    "        labels = np.append(labels, labels[i])\n",
    "        images.append(np.rot90(images[i], 1))\n",
    "        labels = np.append(labels, labels[i])\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    # CONVERT TO NUMPY ARRAY\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "train_images, train_labels = load_data_from_folder('training-a', 'training-a.csv')\n",
    "\n",
    "# reshape images to 28x28x1\n",
    "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2800, 28, 28, 1)\n",
      "(2800,)\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# print shape of images and labels\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "\n",
    "#print shape of first image\n",
    "print(train_images[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize images with std and mean\n",
    "train_images = (train_images - np.mean(train_images)) / np.std(train_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and validation\n",
    "train_percent = 0.8\n",
    "X_train = train_images[:int(train_percent*len(train_images))]\n",
    "y_train = train_labels[:int(train_percent*len(train_labels))]\n",
    "X_val = train_images[int(train_percent*len(train_images)):]\n",
    "y_val = train_labels[int(train_percent*len(train_labels)):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model class definition\n",
    "class Model:\n",
    "    def __init__(self, num_classes):\n",
    "        self.layers = []\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def forward(self, input):\n",
    "        for layer in self.layers:\n",
    "            input = layer.forward(input)\n",
    "        return input\n",
    "\n",
    "    def one_hot_encode(self, labels):\n",
    "        one_hot = np.zeros((len(labels), self.num_classes))\n",
    "        for i, label in enumerate(labels):\n",
    "            one_hot[i][label] = 1\n",
    "        return one_hot\n",
    "    \n",
    "    def cross_entropy(self, y_true, y_pred):\n",
    "        eps = 1e-9\n",
    "        return -np.sum(y_true * np.log(y_pred + eps))\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.forward(X)\n",
    "        return np.argmax(y_pred, axis=1)\n",
    "\n",
    "    def evaluate(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        #print (y_pred), (y)\n",
    "        print(\"y_pred: \", y_pred)\n",
    "        print(\"y: \", y)\n",
    "        accuracy = np.mean(y_pred == y)\n",
    "        return accuracy \n",
    "\n",
    "    def train(self, X_train, y_train, X_val, y_val, learning_rate, epochs, batch_size):\n",
    "        for epoch in range(epochs):\n",
    "            print(\"epoch: \", epoch)\n",
    "\n",
    "            # split data into batches\n",
    "            batches = []\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                batches.append((X_train[i:i+batch_size], y_train[i:i+batch_size]))\n",
    "\n",
    "            # train model\n",
    "            for i in range(len(batches)):\n",
    "                X_batch, y_batch = batches[i]\n",
    "                print(\"\\tbatch: \", i)\n",
    "                y_batch_one_hot = self.one_hot_encode(y_batch)\n",
    "                y_pred = self.forward(X_batch)\n",
    "                loss = self.cross_entropy(y_batch_one_hot, y_pred)\n",
    "                print(\"\\t\\tloss: \", loss)\n",
    "                grad = y_pred - y_batch_one_hot\n",
    "                for layer in reversed(self.layers):\n",
    "                    grad = layer.backprop(grad, learning_rate)\n",
    "            \n",
    "            # evaluate model\n",
    "            accuracy = self.evaluate(X_val, y_val)\n",
    "            print(\"accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# use lenet-5 model\n",
    "model = Model(10)\n",
    "model.add(ConvolutionLayer(6, 5, 1, 1))\n",
    "model.add(ReLULayer())\n",
    "model.add(MaxPoolingLayer(pool_size=2, stride=2))\n",
    "model.add(ConvolutionLayer(16, 5, 1, 1))\n",
    "model.add(ReLULayer())\n",
    "model.add(MaxPoolingLayer(pool_size=2, stride=2))\n",
    "model.add(FlattenLayer())\n",
    "model.add(FullyConnectedLayer(output_size=120))\n",
    "model.add(ReLULayer())\n",
    "model.add(FullyConnectedLayer(output_size=84))\n",
    "model.add(ReLULayer())\n",
    "model.add(FullyConnectedLayer(output_size=10))\n",
    "model.add(SoftmaxLayer())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model = Model(10)\n",
    "# model.add(ConvolutionLayer(6, 6, 1, 1))\n",
    "# model.add(ReLULayer())\n",
    "# model.add(MaxPoolingLayer(2, 2))\n",
    "# model.add(ConvolutionLayer(16, 10, 1, 1))\n",
    "# model.add(ReLULayer())\n",
    "# model.add(MaxPoolingLayer(2, 2))\n",
    "# model.add(FlattenLayer())\n",
    "# model.add(FullyConnectedLayer(90))\n",
    "# model.add(ReLULayer())\n",
    "# model.add(FullyConnectedLayer(74))\n",
    "# model.add(ReLULayer())\n",
    "# model.add(FullyConnectedLayer(10))\n",
    "# model.add(ReLULayer())\n",
    "# model.add(SoftmaxLayer())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "\tbatch:  0\n",
      "\t\tloss:  2321.0057737219977\n",
      "\tbatch:  1\n",
      "\t\tloss:  2341.7290395599443\n",
      "\tbatch:  2\n",
      "\t\tloss:  2397.2724347826897\n",
      "\tbatch:  3\n",
      "\t\tloss:  2383.175571235837\n",
      "\tbatch:  4\n",
      "\t\tloss:  2286.148265269617\n",
      "\tbatch:  5\n",
      "\t\tloss:  2265.869516824513\n",
      "\tbatch:  6\n",
      "\t\tloss:  2196.6661786943196\n",
      "\tbatch:  7\n",
      "\t\tloss:  2218.57000851474\n",
      "\tbatch:  8\n",
      "\t\tloss:  2351.921880914376\n",
      "\tbatch:  9\n",
      "\t\tloss:  2403.8988370737834\n",
      "\tbatch:  10\n",
      "\t\tloss:  2528.253642556271\n",
      "\tbatch:  11\n",
      "\t\tloss:  2548.884480081096\n",
      "\tbatch:  12\n",
      "\t\tloss:  2279.5592420473404\n",
      "\tbatch:  13\n",
      "\t\tloss:  2466.0686106298044\n",
      "\tbatch:  14\n",
      "\t\tloss:  2403.898837073092\n",
      "\tbatch:  15\n",
      "\t\tloss:  2341.729039551882\n",
      "\tbatch:  16\n",
      "\t\tloss:  2155.219680617822\n",
      "\tbatch:  17\n",
      "\t\tloss:  1077.6098283181811\n",
      "y_pred:  [2 2 8 8 8 8 8 8 2 2 2 2 2 2 4 4 4 3 3 3 1 1 1 1 1 1 2 2 2 2 2 2 6 6 6 7 7\n",
      " 7 2 2 2 6 6 6 1 1 1 2 2 2 4 4 4 1 1 1 2 2 2 4 4 4 1 1 1 4 4 4 2 2 2 1 1 1\n",
      " 7 7 7 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 2 2 2 2 2 2 4 4 4 2 2 2 2 2 2 2\n",
      " 2 2 5 5 5 5 5 5 2 2 2 0 0 0 2 2 2 2 2 2 2 2 2 2 2 2 8 8 8 2 2 2 1 1 1 5 5\n",
      " 5 2 2 2 1 1 1 9 9 9 1 1 1 2 2 2 2 2 2 6 6 6 3 3 3 4 4 4 7 7 7 4 4 4 1 1 1\n",
      " 7 7 7 0 0 0 2 2 2 2 2 2 5 5 5 5 5 5 2 2 2 1 1 1 1 1 1 9 9 9 2 2 2 7 7 7 6\n",
      " 6 6 2 2 2 2 2 2 2 2 2 6 6 6 1 1 1 3 3 3 4 4 4 4 4 4 8 8 8 6 6 6 2 2 2 1 1\n",
      " 1 4 4 4 1 1 1 6 6 6 4 4 4 1 1 1 4 4 4 1 1 1 4 4 4 4 4 4 1 1 1 8 8 8 1 1 1\n",
      " 1 1 1 1 1 1 2 2 2 1 1 1 6 6 6 2 2 2 4 4 4 4 4 4 4 4 4 1 1 1 1 1 1 4 4 4 1\n",
      " 1 1 6 6 6 9 9 9 6 6 6 2 2 2 1 1 1 4 4 4 7 7 7 2 2 2 1 1 1 4 4 4 2 2 2 1 1\n",
      " 1 4 4 4 5 5 5 2 2 2 6 6 6 4 4 4 4 4 4 6 6 6 5 5 5 2 2 2 6 6 6 9 9 9 8 8 8\n",
      " 1 1 1 7 7 7 8 8 8 3 3 3 6 6 6 1 1 1 4 4 4 2 2 2 1 1 1 1 1 1 1 1 1 2 2 2 1\n",
      " 1 1 1 1 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 6 6 6 1 1 1 2 2 2 6 6 6 3 3 3 4 4\n",
      " 4 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 5 5 5 1 1 1 6 6 6 2 2 2 1 1 1 2 2 2 4 4 4\n",
      " 2 2 2 2 2 2 2 2 2 3 3 3 1 1 1 2 2 2 1 1 1 5 5 5 2 2 2 2 2 2 1 1 1 4 4 4 1\n",
      " 1 1 2 2 2]\n",
      "y:  [0 0 0 0 0 9 9 9 7 7 7 7 7 7 3 3 3 6 6 6 3 3 3 5 5 5 1 1 1 9 9 9 7 7 7 6 6\n",
      " 6 2 2 2 0 0 0 4 4 4 2 2 2 2 2 2 3 3 3 9 9 9 9 9 9 7 7 7 2 2 2 7 7 7 0 0 0\n",
      " 9 9 9 3 3 3 2 2 2 5 5 5 3 3 3 4 4 4 6 6 6 2 2 2 3 3 3 4 4 4 9 9 9 5 5 5 0\n",
      " 0 0 4 4 4 3 3 3 2 2 2 3 3 3 1 1 1 1 1 1 6 6 6 7 7 7 4 4 4 2 2 2 1 1 1 7 7\n",
      " 7 5 5 5 2 2 2 7 7 7 9 9 9 1 1 1 4 4 4 3 3 3 1 1 1 2 2 2 5 5 5 8 8 8 7 7 7\n",
      " 8 8 8 7 7 7 7 7 7 9 9 9 5 5 5 0 0 0 9 9 9 8 8 8 2 2 2 7 7 7 2 2 2 8 8 8 4\n",
      " 4 4 9 9 9 1 1 1 0 0 0 4 4 4 2 2 2 1 1 1 0 0 0 2 2 2 3 3 3 6 6 6 2 2 2 3 3\n",
      " 3 5 5 5 2 2 2 8 8 8 5 5 5 5 5 5 5 5 5 6 6 6 4 4 4 7 7 7 5 5 5 8 8 8 9 9 9\n",
      " 5 5 5 4 4 4 9 9 9 6 6 6 1 1 1 0 0 0 4 4 4 6 6 6 3 3 3 3 3 3 8 8 8 4 4 4 5\n",
      " 5 5 0 0 0 3 3 3 2 2 2 3 3 3 5 5 5 4 4 4 8 8 8 2 2 2 0 0 0 4 4 4 1 1 1 4 4\n",
      " 4 0 0 0 7 7 7 2 2 2 5 5 5 8 8 8 6 6 6 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 9 9 9\n",
      " 7 7 7 1 1 1 1 1 1 9 9 9 3 3 3 0 0 0 8 8 8 7 7 7 8 8 8 6 6 6 0 0 0 1 1 1 4\n",
      " 4 4 0 0 0 9 9 9 2 2 2 5 5 5 0 0 0 6 6 6 3 3 3 5 5 5 7 7 7 2 2 2 3 3 3 9 9\n",
      " 9 8 8 8 5 5 5 9 9 9 9 9 9 7 7 7 3 3 3 0 0 0 9 9 9 9 9 9 1 1 1 8 8 8 4 4 4\n",
      " 1 1 1 0 0 0 9 9 9 7 7 7 6 6 6 4 4 4 9 9 9 8 8 8 9 9 9 6 6 6 7 7 7 5 5 5 9\n",
      " 9 9 4 4 4]\n",
      "accuracy:  0.12857142857142856\n",
      "epoch:  1\n",
      "\tbatch:  0\n",
      "\t\tloss:  2305.669855345848\n",
      "\tbatch:  1\n",
      "\t\tloss:  2424.606401106518\n",
      "\tbatch:  2\n",
      "\t\tloss:  2466.068634587623\n",
      "\tbatch:  3\n",
      "\t\tloss:  2454.5356980160777\n",
      "\tbatch:  4\n",
      "\t\tloss:  2436.0895766045596\n",
      "\tbatch:  5\n",
      "\t\tloss:  2238.11271035946\n",
      "\tbatch:  6\n",
      "\t\tloss:  2258.835976208159\n",
      "\tbatch:  7\n",
      "\t\tloss:  2279.613375242208\n",
      "\tbatch:  8\n",
      "\t\tloss:  2359.4241429925896\n",
      "\tbatch:  9\n",
      "\t\tloss:  2403.898837073782\n",
      "\tbatch:  10\n",
      "\t\tloss:  2279.5592420459957\n",
      "\tbatch:  11\n",
      "\t\tloss:  2217.389444749025\n",
      "\tbatch:  12\n",
      "\t\tloss:  2403.898837073779\n",
      "\tbatch:  13\n",
      "\t\tloss:  2589.7531247988163\n",
      "\tbatch:  14\n",
      "\t\tloss:  2403.8988370737834\n",
      "\tbatch:  15\n",
      "\t\tloss:  2279.559242046067\n",
      "\tbatch:  16\n",
      "\t\tloss:  2403.8988357454127\n",
      "\tbatch:  17\n",
      "\t\tloss:  1139.7796210230522\n",
      "y_pred:  [2 2 8 8 8 8 8 8 2 2 2 7 7 7 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 6 6 6 3 3\n",
      " 3 8 8 8 3 3 3 1 1 1 1 1 1 2 2 2 3 3 3 7 7 7 9 9 9 1 1 1 7 7 7 2 2 2 1 1 1\n",
      " 6 6 6 1 1 1 1 1 1 1 1 1 4 4 4 4 4 4 4 4 4 1 1 1 2 2 2 4 4 4 1 1 1 3 3 3 2\n",
      " 2 2 5 5 5 3 3 3 5 5 5 0 0 0 7 7 7 7 7 7 1 1 1 6 6 6 2 2 2 1 1 1 1 1 1 5 5\n",
      " 5 4 4 4 1 1 1 9 9 9 1 1 1 9 9 9 2 2 2 5 5 5 3 3 3 2 2 2 7 7 7 4 4 4 9 9 9\n",
      " 7 7 7 6 6 6 7 7 7 8 8 8 5 5 5 5 5 5 2 2 2 1 1 1 1 1 1 9 9 9 1 1 1 7 7 7 5\n",
      " 5 5 7 7 7 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 4 4 4 6 6 6 8 8 8 6 6 6 2 2 2 7 7\n",
      " 7 4 4 4 1 1 1 4 4 4 7 7 7 1 1 1 4 4 4 1 1 1 2 2 2 6 6 6 6 6 6 4 4 4 8 8 8\n",
      " 1 1 1 1 1 1 9 9 9 1 1 1 6 6 6 9 9 9 7 7 7 8 8 8 4 4 4 1 1 1 1 1 1 3 3 3 8\n",
      " 8 8 0 0 0 7 7 7 6 6 6 1 1 1 1 1 1 4 4 4 7 7 7 2 2 2 1 1 1 1 1 1 7 7 7 3 3\n",
      " 3 4 4 4 9 9 9 1 1 1 6 6 6 4 4 4 2 2 2 7 7 7 5 5 5 2 2 2 4 4 4 7 7 7 8 8 8\n",
      " 1 1 1 9 9 9 8 8 8 3 3 3 6 6 6 5 5 5 4 4 4 1 1 1 1 1 1 1 1 1 9 9 9 3 3 3 1\n",
      " 1 1 6 6 6 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 6 6 6 1 1 1 0 0 0 7 7 7 3 3 3 8 8\n",
      " 8 8 8 8 2 2 2 0 0 0 2 2 2 5 5 5 5 5 5 9 9 9 6 6 6 9 9 9 1 1 1 7 7 7 4 4 4\n",
      " 8 8 8 9 9 9 6 6 6 3 3 3 6 6 6 2 2 2 6 6 6 5 5 5 3 3 3 5 5 5 7 7 7 7 7 7 1\n",
      " 1 1 4 4 4]\n",
      "y:  [0 0 0 0 0 9 9 9 7 7 7 7 7 7 3 3 3 6 6 6 3 3 3 5 5 5 1 1 1 9 9 9 7 7 7 6 6\n",
      " 6 2 2 2 0 0 0 4 4 4 2 2 2 2 2 2 3 3 3 9 9 9 9 9 9 7 7 7 2 2 2 7 7 7 0 0 0\n",
      " 9 9 9 3 3 3 2 2 2 5 5 5 3 3 3 4 4 4 6 6 6 2 2 2 3 3 3 4 4 4 9 9 9 5 5 5 0\n",
      " 0 0 4 4 4 3 3 3 2 2 2 3 3 3 1 1 1 1 1 1 6 6 6 7 7 7 4 4 4 2 2 2 1 1 1 7 7\n",
      " 7 5 5 5 2 2 2 7 7 7 9 9 9 1 1 1 4 4 4 3 3 3 1 1 1 2 2 2 5 5 5 8 8 8 7 7 7\n",
      " 8 8 8 7 7 7 7 7 7 9 9 9 5 5 5 0 0 0 9 9 9 8 8 8 2 2 2 7 7 7 2 2 2 8 8 8 4\n",
      " 4 4 9 9 9 1 1 1 0 0 0 4 4 4 2 2 2 1 1 1 0 0 0 2 2 2 3 3 3 6 6 6 2 2 2 3 3\n",
      " 3 5 5 5 2 2 2 8 8 8 5 5 5 5 5 5 5 5 5 6 6 6 4 4 4 7 7 7 5 5 5 8 8 8 9 9 9\n",
      " 5 5 5 4 4 4 9 9 9 6 6 6 1 1 1 0 0 0 4 4 4 6 6 6 3 3 3 3 3 3 8 8 8 4 4 4 5\n",
      " 5 5 0 0 0 3 3 3 2 2 2 3 3 3 5 5 5 4 4 4 8 8 8 2 2 2 0 0 0 4 4 4 1 1 1 4 4\n",
      " 4 0 0 0 7 7 7 2 2 2 5 5 5 8 8 8 6 6 6 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 9 9 9\n",
      " 7 7 7 1 1 1 1 1 1 9 9 9 3 3 3 0 0 0 8 8 8 7 7 7 8 8 8 6 6 6 0 0 0 1 1 1 4\n",
      " 4 4 0 0 0 9 9 9 2 2 2 5 5 5 0 0 0 6 6 6 3 3 3 5 5 5 7 7 7 2 2 2 3 3 3 9 9\n",
      " 9 8 8 8 5 5 5 9 9 9 9 9 9 7 7 7 3 3 3 0 0 0 9 9 9 9 9 9 1 1 1 8 8 8 4 4 4\n",
      " 1 1 1 0 0 0 9 9 9 7 7 7 6 6 6 4 4 4 9 9 9 8 8 8 9 9 9 6 6 6 7 7 7 5 5 5 9\n",
      " 9 9 4 4 4]\n",
      "accuracy:  0.15535714285714286\n",
      "epoch:  2\n",
      "\tbatch:  0\n",
      "\t\tloss:  2393.380217307864\n",
      "\tbatch:  1\n",
      "\t\tloss:  2320.3238098448\n",
      "\tbatch:  2\n",
      "\t\tloss:  2373.36809439499\n",
      "\tbatch:  3\n",
      "\t\tloss:  2282.8530118631984\n",
      "\tbatch:  4\n",
      "\t\tloss:  2334.079081259683\n",
      "\tbatch:  5\n",
      "\t\tloss:  2112.9569660774573\n",
      "\tbatch:  6\n",
      "\t\tloss:  2265.626686636157\n",
      "\tbatch:  7\n",
      "\t\tloss:  2362.4563697627214\n",
      "\tbatch:  8\n",
      "\t\tloss:  2276.648551513871\n",
      "\tbatch:  9\n",
      "\t\tloss:  2528.238432114813\n",
      "\tbatch:  10\n",
      "\t\tloss:  2061.6705214042368\n",
      "\tbatch:  11\n",
      "\t\tloss:  2207.9806821457523\n",
      "\tbatch:  12\n",
      "\t\tloss:  2195.527813571442\n",
      "\tbatch:  13\n",
      "\t\tloss:  2200.0270196120287\n",
      "\tbatch:  14\n",
      "\t\tloss:  2102.214795972961\n",
      "\tbatch:  15\n",
      "\t\tloss:  2178.7643604473537\n",
      "\tbatch:  16\n",
      "\t\tloss:  2178.901338567399\n",
      "\tbatch:  17\n",
      "\t\tloss:  1023.4244880231535\n",
      "y_pred:  [9 9 8 8 8 8 8 8 2 2 2 7 7 7 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 6 6 6 5 5\n",
      " 5 8 8 8 8 8 8 1 1 1 1 1 1 0 0 0 3 3 3 7 7 7 9 9 9 1 1 1 0 0 0 1 1 1 1 1 1\n",
      " 7 7 7 1 1 1 1 1 1 3 3 3 3 3 3 5 5 5 4 4 4 3 3 3 2 2 2 2 2 2 9 9 9 3 3 3 2\n",
      " 2 2 5 5 5 3 3 3 5 5 5 0 0 0 3 3 3 7 7 7 1 1 1 6 6 6 2 2 2 1 1 1 1 1 1 5 5\n",
      " 5 4 4 4 1 1 1 9 9 9 6 6 6 9 9 9 2 2 2 5 5 5 3 3 3 2 2 2 5 5 5 0 0 0 9 9 9\n",
      " 7 7 7 6 6 6 7 7 7 2 2 2 5 5 5 5 5 5 2 2 2 1 1 1 1 1 1 9 9 9 9 9 9 5 5 5 5\n",
      " 5 5 7 7 7 1 1 1 8 8 8 1 1 1 9 9 9 3 3 3 7 7 7 4 4 4 8 8 8 6 6 6 3 3 3 0 0\n",
      " 0 4 4 4 1 1 1 4 4 4 2 2 2 3 3 3 4 4 4 4 4 4 2 2 2 6 6 6 4 4 4 4 4 4 7 7 7\n",
      " 1 1 1 1 1 1 9 9 9 1 1 1 6 6 6 9 9 9 4 4 4 6 6 6 5 5 5 1 1 1 9 9 9 3 3 3 4\n",
      " 4 4 6 6 6 7 7 7 6 6 6 1 1 1 4 4 4 7 7 7 7 7 7 3 3 3 1 1 1 4 4 4 7 7 7 3 3\n",
      " 3 1 1 1 9 9 9 1 1 1 5 5 5 4 4 4 3 3 3 7 7 7 8 8 8 2 2 2 9 9 9 3 3 3 8 8 8\n",
      " 1 1 1 9 9 9 1 1 1 3 3 3 6 6 6 5 5 5 5 5 5 2 2 2 8 8 8 1 1 1 9 9 9 2 2 2 6\n",
      " 6 6 6 6 6 1 1 1 2 2 2 1 1 1 1 1 1 1 1 1 6 6 6 1 1 1 0 0 0 7 7 7 3 3 3 8 8\n",
      " 8 1 1 1 2 2 2 0 0 0 2 2 2 3 3 3 5 5 5 9 9 9 9 9 9 9 9 9 0 0 0 7 7 7 2 2 2\n",
      " 2 2 2 9 9 9 2 2 2 6 6 6 6 6 6 3 3 3 6 6 6 8 8 8 3 3 3 4 4 4 7 7 7 8 8 8 1\n",
      " 1 1 4 4 4]\n",
      "y:  [0 0 0 0 0 9 9 9 7 7 7 7 7 7 3 3 3 6 6 6 3 3 3 5 5 5 1 1 1 9 9 9 7 7 7 6 6\n",
      " 6 2 2 2 0 0 0 4 4 4 2 2 2 2 2 2 3 3 3 9 9 9 9 9 9 7 7 7 2 2 2 7 7 7 0 0 0\n",
      " 9 9 9 3 3 3 2 2 2 5 5 5 3 3 3 4 4 4 6 6 6 2 2 2 3 3 3 4 4 4 9 9 9 5 5 5 0\n",
      " 0 0 4 4 4 3 3 3 2 2 2 3 3 3 1 1 1 1 1 1 6 6 6 7 7 7 4 4 4 2 2 2 1 1 1 7 7\n",
      " 7 5 5 5 2 2 2 7 7 7 9 9 9 1 1 1 4 4 4 3 3 3 1 1 1 2 2 2 5 5 5 8 8 8 7 7 7\n",
      " 8 8 8 7 7 7 7 7 7 9 9 9 5 5 5 0 0 0 9 9 9 8 8 8 2 2 2 7 7 7 2 2 2 8 8 8 4\n",
      " 4 4 9 9 9 1 1 1 0 0 0 4 4 4 2 2 2 1 1 1 0 0 0 2 2 2 3 3 3 6 6 6 2 2 2 3 3\n",
      " 3 5 5 5 2 2 2 8 8 8 5 5 5 5 5 5 5 5 5 6 6 6 4 4 4 7 7 7 5 5 5 8 8 8 9 9 9\n",
      " 5 5 5 4 4 4 9 9 9 6 6 6 1 1 1 0 0 0 4 4 4 6 6 6 3 3 3 3 3 3 8 8 8 4 4 4 5\n",
      " 5 5 0 0 0 3 3 3 2 2 2 3 3 3 5 5 5 4 4 4 8 8 8 2 2 2 0 0 0 4 4 4 1 1 1 4 4\n",
      " 4 0 0 0 7 7 7 2 2 2 5 5 5 8 8 8 6 6 6 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 9 9 9\n",
      " 7 7 7 1 1 1 1 1 1 9 9 9 3 3 3 0 0 0 8 8 8 7 7 7 8 8 8 6 6 6 0 0 0 1 1 1 4\n",
      " 4 4 0 0 0 9 9 9 2 2 2 5 5 5 0 0 0 6 6 6 3 3 3 5 5 5 7 7 7 2 2 2 3 3 3 9 9\n",
      " 9 8 8 8 5 5 5 9 9 9 9 9 9 7 7 7 3 3 3 0 0 0 9 9 9 9 9 9 1 1 1 8 8 8 4 4 4\n",
      " 1 1 1 0 0 0 9 9 9 7 7 7 6 6 6 4 4 4 9 9 9 8 8 8 9 9 9 6 6 6 7 7 7 5 5 5 9\n",
      " 9 9 4 4 4]\n",
      "accuracy:  0.16607142857142856\n",
      "epoch:  3\n",
      "\tbatch:  0\n",
      "\t\tloss:  2360.376427143142\n",
      "\tbatch:  1\n",
      "\t\tloss:  2320.5966149600254\n",
      "\tbatch:  2\n",
      "\t\tloss:  2373.687465926083\n",
      "\tbatch:  3\n",
      "\t\tloss:  2184.7797287111375\n",
      "\tbatch:  4\n",
      "\t\tloss:  2300.691463337745\n",
      "\tbatch:  5\n",
      "\t\tloss:  2007.3368211566626\n",
      "\tbatch:  6\n",
      "\t\tloss:  1985.7214444739423\n",
      "\tbatch:  7\n",
      "\t\tloss:  2275.4118306193436\n",
      "\tbatch:  8\n",
      "\t\tloss:  2279.5592380178614\n",
      "\tbatch:  9\n",
      "\t\tloss:  2217.3894445322658\n",
      "\tbatch:  10\n",
      "\t\tloss:  1986.3055515577662\n",
      "\tbatch:  11\n",
      "\t\tloss:  1978.2003973407473\n",
      "\tbatch:  12\n",
      "\t\tloss:  2196.3373364739464\n",
      "\tbatch:  13\n",
      "\t\tloss:  1929.7254167169706\n",
      "\tbatch:  14\n",
      "\t\tloss:  2028.5589102732251\n",
      "\tbatch:  15\n",
      "\t\tloss:  2205.8547229429328\n",
      "\tbatch:  16\n",
      "\t\tloss:  2040.526945667887\n",
      "\tbatch:  17\n",
      "\t\tloss:  1015.4400259953741\n",
      "y_pred:  [2 2 8 8 8 8 8 8 9 9 9 7 7 7 3 3 3 3 3 3 1 1 1 4 4 4 1 1 1 1 1 1 6 6 6 7 7\n",
      " 7 8 8 8 8 8 8 4 4 4 1 1 1 0 0 0 1 1 1 7 7 7 1 1 1 1 1 1 7 7 7 1 1 1 0 0 0\n",
      " 7 7 7 1 1 1 9 9 9 3 3 3 3 3 3 5 5 5 4 4 4 8 8 8 3 3 3 2 2 2 9 9 9 3 3 3 2\n",
      " 2 2 5 5 5 3 3 3 3 3 3 0 0 0 8 8 8 5 5 5 5 5 5 9 9 9 2 2 2 4 4 4 1 1 1 1 1\n",
      " 1 4 4 4 9 9 9 9 9 9 0 0 0 9 9 9 3 3 3 5 5 5 3 3 3 4 4 4 5 5 5 9 9 9 9 9 9\n",
      " 7 7 7 0 0 0 9 9 9 2 2 2 5 5 5 8 8 8 2 2 2 0 0 0 8 8 8 9 9 9 9 9 9 7 7 7 5\n",
      " 5 5 3 3 3 9 9 9 8 8 8 1 1 1 9 9 9 3 3 3 1 1 1 4 4 4 8 8 8 4 4 4 3 3 3 0 0\n",
      " 0 5 5 5 0 0 0 3 3 3 2 2 2 3 3 3 3 3 3 4 4 4 2 2 2 6 6 6 4 4 4 8 8 8 8 8 8\n",
      " 4 4 4 5 5 5 9 9 9 4 4 4 5 5 5 3 3 3 4 4 4 8 8 8 3 3 3 1 1 1 9 9 9 3 3 3 7\n",
      " 7 7 6 6 6 7 7 7 3 3 3 0 0 0 4 4 4 7 7 7 5 5 5 3 3 3 8 8 8 4 4 4 7 7 7 4 4\n",
      " 4 8 8 8 9 9 9 2 2 2 5 5 5 8 8 8 3 3 3 7 7 7 8 8 8 2 2 2 9 9 9 3 3 3 8 8 8\n",
      " 1 1 1 9 9 9 2 2 2 3 3 3 6 6 6 5 5 5 8 8 8 0 0 0 8 8 8 5 5 5 9 9 9 2 2 2 6\n",
      " 6 6 6 6 6 9 9 9 2 2 2 5 5 5 1 1 1 1 1 1 4 4 4 8 8 8 0 0 0 7 7 7 9 9 9 8 8\n",
      " 8 1 1 1 8 8 8 0 0 0 0 0 0 3 3 3 5 5 5 9 9 9 9 9 9 9 9 9 0 0 0 1 1 1 2 2 2\n",
      " 8 8 8 2 2 2 0 0 0 6 6 6 6 6 6 8 8 8 9 9 9 8 8 8 2 2 2 4 4 4 7 7 7 5 5 5 0\n",
      " 0 0 4 4 4]\n",
      "y:  [0 0 0 0 0 9 9 9 7 7 7 7 7 7 3 3 3 6 6 6 3 3 3 5 5 5 1 1 1 9 9 9 7 7 7 6 6\n",
      " 6 2 2 2 0 0 0 4 4 4 2 2 2 2 2 2 3 3 3 9 9 9 9 9 9 7 7 7 2 2 2 7 7 7 0 0 0\n",
      " 9 9 9 3 3 3 2 2 2 5 5 5 3 3 3 4 4 4 6 6 6 2 2 2 3 3 3 4 4 4 9 9 9 5 5 5 0\n",
      " 0 0 4 4 4 3 3 3 2 2 2 3 3 3 1 1 1 1 1 1 6 6 6 7 7 7 4 4 4 2 2 2 1 1 1 7 7\n",
      " 7 5 5 5 2 2 2 7 7 7 9 9 9 1 1 1 4 4 4 3 3 3 1 1 1 2 2 2 5 5 5 8 8 8 7 7 7\n",
      " 8 8 8 7 7 7 7 7 7 9 9 9 5 5 5 0 0 0 9 9 9 8 8 8 2 2 2 7 7 7 2 2 2 8 8 8 4\n",
      " 4 4 9 9 9 1 1 1 0 0 0 4 4 4 2 2 2 1 1 1 0 0 0 2 2 2 3 3 3 6 6 6 2 2 2 3 3\n",
      " 3 5 5 5 2 2 2 8 8 8 5 5 5 5 5 5 5 5 5 6 6 6 4 4 4 7 7 7 5 5 5 8 8 8 9 9 9\n",
      " 5 5 5 4 4 4 9 9 9 6 6 6 1 1 1 0 0 0 4 4 4 6 6 6 3 3 3 3 3 3 8 8 8 4 4 4 5\n",
      " 5 5 0 0 0 3 3 3 2 2 2 3 3 3 5 5 5 4 4 4 8 8 8 2 2 2 0 0 0 4 4 4 1 1 1 4 4\n",
      " 4 0 0 0 7 7 7 2 2 2 5 5 5 8 8 8 6 6 6 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 9 9 9\n",
      " 7 7 7 1 1 1 1 1 1 9 9 9 3 3 3 0 0 0 8 8 8 7 7 7 8 8 8 6 6 6 0 0 0 1 1 1 4\n",
      " 4 4 0 0 0 9 9 9 2 2 2 5 5 5 0 0 0 6 6 6 3 3 3 5 5 5 7 7 7 2 2 2 3 3 3 9 9\n",
      " 9 8 8 8 5 5 5 9 9 9 9 9 9 7 7 7 3 3 3 0 0 0 9 9 9 9 9 9 1 1 1 8 8 8 4 4 4\n",
      " 1 1 1 0 0 0 9 9 9 7 7 7 6 6 6 4 4 4 9 9 9 8 8 8 9 9 9 6 6 6 7 7 7 5 5 5 9\n",
      " 9 9 4 4 4]\n",
      "accuracy:  0.19285714285714287\n",
      "epoch:  4\n",
      "\tbatch:  0\n",
      "\t\tloss:  2321.0057416582117\n",
      "\tbatch:  1\n",
      "\t\tloss:  2292.20266554073\n",
      "\tbatch:  2\n",
      "\t\tloss:  2374.7939266264752\n",
      "\tbatch:  3\n",
      "\t\tloss:  2217.3752395528672\n",
      "\tbatch:  4\n",
      "\t\tloss:  2332.579706198851\n",
      "\tbatch:  5\n",
      "\t\tloss:  2170.722048977097\n",
      "\tbatch:  6\n",
      "\t\tloss:  2093.05231889468\n",
      "\tbatch:  7\n",
      "\t\tloss:  2216.5721728376525\n",
      "\tbatch:  8\n",
      "\t\tloss:  2099.1571411434848\n",
      "\tbatch:  9\n",
      "\t\tloss:  2217.389444569908\n",
      "\tbatch:  10\n",
      "\t\tloss:  1906.540463151526\n",
      "\tbatch:  11\n",
      "\t\tloss:  2093.0440068259713\n",
      "\tbatch:  12\n",
      "\t\tloss:  2314.9976310623897\n",
      "\tbatch:  13\n",
      "\t\tloss:  1919.5804960447235\n",
      "\tbatch:  14\n",
      "\t\tloss:  1844.6387981731837\n",
      "\tbatch:  15\n",
      "\t\tloss:  2217.3895008616473\n",
      "\tbatch:  16\n",
      "\t\tloss:  2279.5592324020695\n",
      "\tbatch:  17\n",
      "\t\tloss:  1015.439850105721\n",
      "y_pred:  [2 2 8 8 8 1 1 1 5 5 5 7 7 7 3 3 3 7 7 7 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 7 7\n",
      " 7 8 8 8 3 3 3 1 1 1 2 2 2 0 0 0 1 1 1 7 7 7 1 1 1 1 1 1 7 7 7 1 1 1 0 0 0\n",
      " 9 9 9 1 1 1 9 9 9 6 6 6 3 3 3 5 5 5 4 4 4 2 2 2 3 3 3 2 2 2 9 9 9 3 3 3 2\n",
      " 2 2 5 5 5 3 3 3 3 3 3 0 0 0 8 8 8 5 5 5 3 3 3 9 9 9 8 8 8 4 4 4 1 1 1 1 1\n",
      " 1 4 4 4 9 9 9 7 7 7 1 1 1 9 9 9 5 5 5 5 5 5 3 3 3 8 8 8 5 5 5 7 7 7 9 9 9\n",
      " 7 7 7 0 0 0 9 9 9 5 5 5 5 5 5 8 8 8 2 2 2 1 1 1 8 8 8 9 9 9 9 9 9 2 2 2 5\n",
      " 5 5 3 3 3 1 1 1 8 8 8 1 1 1 9 9 9 7 7 7 1 1 1 4 4 4 8 8 8 4 4 4 3 3 3 0 0\n",
      " 0 5 5 5 1 1 1 3 3 3 2 2 2 3 3 3 3 3 3 4 4 4 2 2 2 6 6 6 4 4 4 8 8 8 0 0 0\n",
      " 4 4 4 5 5 5 9 9 9 4 4 4 5 5 5 3 3 3 4 4 4 6 6 6 3 3 3 1 1 1 9 9 9 3 3 3 4\n",
      " 4 4 6 6 6 7 7 7 3 3 3 9 9 9 4 4 4 8 8 8 5 5 5 5 5 5 3 3 3 4 4 4 7 7 7 3 3\n",
      " 3 2 2 2 7 7 7 2 2 2 5 5 5 8 8 8 3 3 3 9 9 9 7 7 7 2 2 2 9 9 9 3 3 3 8 8 8\n",
      " 1 1 1 9 9 9 9 9 9 3 3 3 6 6 6 5 5 5 3 3 3 2 2 2 8 8 8 1 1 1 1 1 1 2 2 2 6\n",
      " 6 6 6 6 6 9 9 9 2 2 2 5 5 5 1 1 1 5 5 5 4 4 4 1 1 1 0 0 0 7 7 7 9 9 9 8 8\n",
      " 8 1 1 1 3 3 3 0 0 0 0 0 0 3 3 3 5 5 5 9 9 9 0 0 0 9 9 9 0 0 0 1 1 1 4 4 4\n",
      " 9 9 9 9 9 9 9 9 9 3 3 3 6 6 6 3 3 3 9 9 9 8 8 8 3 3 3 5 5 5 7 7 7 5 5 5 0\n",
      " 0 0 5 5 5]\n",
      "y:  [0 0 0 0 0 9 9 9 7 7 7 7 7 7 3 3 3 6 6 6 3 3 3 5 5 5 1 1 1 9 9 9 7 7 7 6 6\n",
      " 6 2 2 2 0 0 0 4 4 4 2 2 2 2 2 2 3 3 3 9 9 9 9 9 9 7 7 7 2 2 2 7 7 7 0 0 0\n",
      " 9 9 9 3 3 3 2 2 2 5 5 5 3 3 3 4 4 4 6 6 6 2 2 2 3 3 3 4 4 4 9 9 9 5 5 5 0\n",
      " 0 0 4 4 4 3 3 3 2 2 2 3 3 3 1 1 1 1 1 1 6 6 6 7 7 7 4 4 4 2 2 2 1 1 1 7 7\n",
      " 7 5 5 5 2 2 2 7 7 7 9 9 9 1 1 1 4 4 4 3 3 3 1 1 1 2 2 2 5 5 5 8 8 8 7 7 7\n",
      " 8 8 8 7 7 7 7 7 7 9 9 9 5 5 5 0 0 0 9 9 9 8 8 8 2 2 2 7 7 7 2 2 2 8 8 8 4\n",
      " 4 4 9 9 9 1 1 1 0 0 0 4 4 4 2 2 2 1 1 1 0 0 0 2 2 2 3 3 3 6 6 6 2 2 2 3 3\n",
      " 3 5 5 5 2 2 2 8 8 8 5 5 5 5 5 5 5 5 5 6 6 6 4 4 4 7 7 7 5 5 5 8 8 8 9 9 9\n",
      " 5 5 5 4 4 4 9 9 9 6 6 6 1 1 1 0 0 0 4 4 4 6 6 6 3 3 3 3 3 3 8 8 8 4 4 4 5\n",
      " 5 5 0 0 0 3 3 3 2 2 2 3 3 3 5 5 5 4 4 4 8 8 8 2 2 2 0 0 0 4 4 4 1 1 1 4 4\n",
      " 4 0 0 0 7 7 7 2 2 2 5 5 5 8 8 8 6 6 6 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 9 9 9\n",
      " 7 7 7 1 1 1 1 1 1 9 9 9 3 3 3 0 0 0 8 8 8 7 7 7 8 8 8 6 6 6 0 0 0 1 1 1 4\n",
      " 4 4 0 0 0 9 9 9 2 2 2 5 5 5 0 0 0 6 6 6 3 3 3 5 5 5 7 7 7 2 2 2 3 3 3 9 9\n",
      " 9 8 8 8 5 5 5 9 9 9 9 9 9 7 7 7 3 3 3 0 0 0 9 9 9 9 9 9 1 1 1 8 8 8 4 4 4\n",
      " 1 1 1 0 0 0 9 9 9 7 7 7 6 6 6 4 4 4 9 9 9 8 8 8 9 9 9 6 6 6 7 7 7 5 5 5 9\n",
      " 9 9 4 4 4]\n",
      "accuracy:  0.20892857142857144\n",
      "epoch:  5\n",
      "\tbatch:  0\n",
      "\t\tloss:  2300.2720722405857\n",
      "\tbatch:  1\n",
      "\t\tloss:  2351.8910387496144\n",
      "\tbatch:  2\n",
      "\t\tloss:  2238.112710370324\n",
      "\tbatch:  3\n",
      "\t\tloss:  2135.49689376364\n",
      "\tbatch:  4\n",
      "\t\tloss:  2335.2565618471904\n",
      "\tbatch:  5\n",
      "\t\tloss:  2355.2236116327867\n",
      "\tbatch:  6\n",
      "\t\tloss:  2066.1873945124244\n",
      "\tbatch:  7\n",
      "\t\tloss:  2054.343717210225\n",
      "\tbatch:  8\n",
      "\t\tloss:  2077.935724950025\n",
      "\tbatch:  9\n",
      "\t\tloss:  2093.0498495048255\n",
      "\tbatch:  10\n",
      "\t\tloss:  1842.3408581591532\n",
      "\tbatch:  11\n",
      "\t\tloss:  1910.6092561077826\n",
      "\tbatch:  12\n",
      "\t\tloss:  2107.191902677801\n",
      "\tbatch:  13\n",
      "\t\tloss:  1830.400683956275\n",
      "\tbatch:  14\n",
      "\t\tloss:  1906.0233749931783\n",
      "\tbatch:  15\n",
      "\t\tloss:  2155.2121788187433\n",
      "\tbatch:  16\n",
      "\t\tloss:  2243.186391281272\n",
      "\tbatch:  17\n",
      "\t\tloss:  906.2201076654926\n",
      "y_pred:  [2 2 8 8 8 1 1 1 5 5 5 7 7 7 3 3 3 3 3 3 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 7 7\n",
      " 7 8 8 8 3 3 3 4 4 4 2 2 2 2 2 2 3 3 3 7 7 7 1 1 1 1 1 1 7 7 7 1 1 1 5 5 5\n",
      " 7 7 7 1 1 1 9 9 9 6 6 6 3 3 3 5 5 5 4 4 4 2 2 2 3 3 3 2 2 2 9 9 9 3 3 3 2\n",
      " 2 2 5 5 5 3 3 3 1 1 1 0 0 0 8 8 8 8 8 8 8 8 8 1 1 1 8 8 8 4 4 4 4 4 4 1 1\n",
      " 1 4 4 4 9 9 9 7 7 7 6 6 6 8 8 8 5 5 5 3 3 3 3 3 3 8 8 8 5 5 5 7 7 7 9 9 9\n",
      " 7 7 7 0 0 0 9 9 9 5 5 5 5 5 5 8 8 8 2 2 2 1 1 1 8 8 8 9 9 9 9 9 9 2 2 2 5\n",
      " 5 5 0 0 0 1 1 1 8 8 8 1 1 1 9 9 9 3 3 3 0 0 0 8 8 8 8 8 8 4 4 4 3 3 3 3 3\n",
      " 3 4 4 4 1 1 1 3 3 3 3 3 3 3 3 3 3 3 3 4 4 4 2 2 2 6 6 6 6 6 6 8 8 8 0 0 0\n",
      " 4 4 4 5 5 5 9 9 9 5 5 5 5 5 5 3 3 3 4 4 4 6 6 6 3 3 3 1 1 1 9 9 9 3 3 3 5\n",
      " 5 5 6 6 6 9 9 9 3 3 3 9 9 9 4 4 4 8 8 8 5 5 5 5 5 5 3 3 3 4 4 4 7 7 7 3 3\n",
      " 3 8 8 8 7 7 7 2 2 2 6 6 6 8 8 8 3 3 3 8 8 8 7 7 7 2 2 2 3 3 3 3 3 3 8 8 8\n",
      " 1 1 1 9 9 9 2 2 2 3 3 3 6 6 6 5 5 5 3 3 3 2 2 2 8 8 8 1 1 1 1 1 1 2 2 2 6\n",
      " 6 6 6 6 6 2 2 2 2 2 2 5 5 5 1 1 1 5 5 5 4 4 4 5 5 5 9 9 9 7 7 7 3 3 3 8 8\n",
      " 8 1 1 1 8 8 8 0 0 0 0 0 0 3 3 3 5 5 5 7 7 7 0 0 0 9 9 9 0 0 0 3 3 3 8 8 8\n",
      " 2 2 2 9 9 9 9 9 9 3 3 3 6 6 6 8 8 8 9 9 9 8 8 8 3 3 3 5 5 5 7 7 7 5 5 5 1\n",
      " 1 1 5 5 5]\n",
      "y:  [0 0 0 0 0 9 9 9 7 7 7 7 7 7 3 3 3 6 6 6 3 3 3 5 5 5 1 1 1 9 9 9 7 7 7 6 6\n",
      " 6 2 2 2 0 0 0 4 4 4 2 2 2 2 2 2 3 3 3 9 9 9 9 9 9 7 7 7 2 2 2 7 7 7 0 0 0\n",
      " 9 9 9 3 3 3 2 2 2 5 5 5 3 3 3 4 4 4 6 6 6 2 2 2 3 3 3 4 4 4 9 9 9 5 5 5 0\n",
      " 0 0 4 4 4 3 3 3 2 2 2 3 3 3 1 1 1 1 1 1 6 6 6 7 7 7 4 4 4 2 2 2 1 1 1 7 7\n",
      " 7 5 5 5 2 2 2 7 7 7 9 9 9 1 1 1 4 4 4 3 3 3 1 1 1 2 2 2 5 5 5 8 8 8 7 7 7\n",
      " 8 8 8 7 7 7 7 7 7 9 9 9 5 5 5 0 0 0 9 9 9 8 8 8 2 2 2 7 7 7 2 2 2 8 8 8 4\n",
      " 4 4 9 9 9 1 1 1 0 0 0 4 4 4 2 2 2 1 1 1 0 0 0 2 2 2 3 3 3 6 6 6 2 2 2 3 3\n",
      " 3 5 5 5 2 2 2 8 8 8 5 5 5 5 5 5 5 5 5 6 6 6 4 4 4 7 7 7 5 5 5 8 8 8 9 9 9\n",
      " 5 5 5 4 4 4 9 9 9 6 6 6 1 1 1 0 0 0 4 4 4 6 6 6 3 3 3 3 3 3 8 8 8 4 4 4 5\n",
      " 5 5 0 0 0 3 3 3 2 2 2 3 3 3 5 5 5 4 4 4 8 8 8 2 2 2 0 0 0 4 4 4 1 1 1 4 4\n",
      " 4 0 0 0 7 7 7 2 2 2 5 5 5 8 8 8 6 6 6 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 9 9 9\n",
      " 7 7 7 1 1 1 1 1 1 9 9 9 3 3 3 0 0 0 8 8 8 7 7 7 8 8 8 6 6 6 0 0 0 1 1 1 4\n",
      " 4 4 0 0 0 9 9 9 2 2 2 5 5 5 0 0 0 6 6 6 3 3 3 5 5 5 7 7 7 2 2 2 3 3 3 9 9\n",
      " 9 8 8 8 5 5 5 9 9 9 9 9 9 7 7 7 3 3 3 0 0 0 9 9 9 9 9 9 1 1 1 8 8 8 4 4 4\n",
      " 1 1 1 0 0 0 9 9 9 7 7 7 6 6 6 4 4 4 9 9 9 8 8 8 9 9 9 6 6 6 7 7 7 5 5 5 9\n",
      " 9 9 4 4 4]\n",
      "accuracy:  0.21964285714285714\n",
      "epoch:  6\n",
      "\tbatch:  0\n",
      "\t\tloss:  2177.017716462341\n",
      "\tbatch:  1\n",
      "\t\tloss:  2295.7987315417313\n",
      "\tbatch:  2\n",
      "\t\tloss:  2226.8593868587004\n",
      "\tbatch:  3\n",
      "\t\tloss:  2169.502461194331\n",
      "\tbatch:  4\n",
      "\t\tloss:  2324.300928638182\n",
      "\tbatch:  5\n",
      "\t\tloss:  2176.4898228763705\n",
      "\tbatch:  6\n",
      "\t\tloss:  2039.9101885050381\n",
      "\tbatch:  7\n",
      "\t\tloss:  1995.4474833912986\n",
      "\tbatch:  8\n",
      "\t\tloss:  2050.312431676626\n",
      "\tbatch:  9\n",
      "\t\tloss:  2149.6166858801794\n",
      "\tbatch:  10\n",
      "\t\tloss:  1945.043536412966\n",
      "\tbatch:  11\n",
      "\t\tloss:  2030.8800519907481\n",
      "\tbatch:  12\n",
      "\t\tloss:  2020.0992400797074\n",
      "\tbatch:  13\n",
      "\t\tloss:  1694.824292134834\n",
      "\tbatch:  14\n",
      "\t\tloss:  1781.876020483095\n",
      "\tbatch:  15\n",
      "\t\tloss:  2203.287727307663\n",
      "\tbatch:  16\n",
      "\t\tloss:  2279.5592420461007\n",
      "\tbatch:  17\n",
      "\t\tloss:  865.1945450094256\n",
      "y_pred:  [9 9 8 8 8 8 8 8 1 1 1 7 7 7 3 3 3 3 3 3 1 1 1 8 8 8 1 1 1 2 2 2 6 6 6 7 7\n",
      " 7 8 8 8 3 3 3 6 6 6 2 2 2 8 8 8 3 3 3 7 7 7 9 9 9 1 1 1 3 3 3 1 1 1 3 3 3\n",
      " 9 9 9 1 1 1 9 9 9 6 6 6 3 3 3 5 5 5 4 4 4 2 2 2 3 3 3 2 2 2 9 9 9 3 3 3 2\n",
      " 2 2 5 5 5 3 3 3 2 2 2 6 6 6 3 3 3 8 8 8 3 3 3 1 1 1 8 8 8 7 7 7 1 1 1 1 1\n",
      " 1 4 4 4 1 1 1 6 6 6 6 6 6 9 9 9 5 5 5 3 3 3 3 3 3 8 8 8 3 3 3 7 7 7 1 1 1\n",
      " 7 7 7 0 0 0 9 9 9 5 5 5 3 3 3 8 8 8 2 2 2 1 1 1 8 8 8 9 9 9 8 8 8 2 2 2 3\n",
      " 3 3 0 0 0 1 1 1 9 9 9 1 1 1 9 9 9 8 8 8 0 0 0 2 2 2 8 8 8 4 4 4 3 3 3 3 3\n",
      " 3 5 5 5 1 1 1 8 8 8 3 3 3 6 6 6 3 3 3 4 4 4 5 5 5 6 6 6 6 6 6 8 8 8 0 0 0\n",
      " 1 1 1 5 5 5 9 9 9 1 1 1 3 3 3 3 3 3 4 4 4 6 6 6 3 3 3 1 1 1 9 9 9 3 3 3 5\n",
      " 5 5 0 0 0 9 9 9 3 3 3 9 9 9 4 4 4 8 8 8 5 5 5 5 5 5 3 3 3 4 4 4 7 7 7 6 6\n",
      " 6 2 2 2 8 8 8 2 2 2 6 6 6 8 8 8 3 3 3 9 9 9 9 9 9 9 9 9 3 3 3 3 3 3 8 8 8\n",
      " 1 1 1 9 9 9 2 2 2 3 3 3 3 3 3 5 5 5 3 3 3 0 0 0 8 8 8 1 1 1 1 1 1 2 2 2 6\n",
      " 6 6 6 6 6 9 9 9 2 2 2 5 5 5 1 1 1 5 5 5 4 4 4 3 3 3 0 0 0 7 7 7 3 3 3 8 8\n",
      " 8 1 1 1 3 3 3 0 0 0 0 0 0 3 3 3 5 5 5 9 9 9 8 8 8 2 2 2 0 0 0 3 3 3 8 8 8\n",
      " 9 9 9 9 9 9 3 3 3 3 3 3 6 6 6 8 8 8 7 7 7 8 8 8 3 3 3 5 5 5 1 1 1 6 6 6 0\n",
      " 0 0 8 8 8]\n",
      "y:  [0 0 0 0 0 9 9 9 7 7 7 7 7 7 3 3 3 6 6 6 3 3 3 5 5 5 1 1 1 9 9 9 7 7 7 6 6\n",
      " 6 2 2 2 0 0 0 4 4 4 2 2 2 2 2 2 3 3 3 9 9 9 9 9 9 7 7 7 2 2 2 7 7 7 0 0 0\n",
      " 9 9 9 3 3 3 2 2 2 5 5 5 3 3 3 4 4 4 6 6 6 2 2 2 3 3 3 4 4 4 9 9 9 5 5 5 0\n",
      " 0 0 4 4 4 3 3 3 2 2 2 3 3 3 1 1 1 1 1 1 6 6 6 7 7 7 4 4 4 2 2 2 1 1 1 7 7\n",
      " 7 5 5 5 2 2 2 7 7 7 9 9 9 1 1 1 4 4 4 3 3 3 1 1 1 2 2 2 5 5 5 8 8 8 7 7 7\n",
      " 8 8 8 7 7 7 7 7 7 9 9 9 5 5 5 0 0 0 9 9 9 8 8 8 2 2 2 7 7 7 2 2 2 8 8 8 4\n",
      " 4 4 9 9 9 1 1 1 0 0 0 4 4 4 2 2 2 1 1 1 0 0 0 2 2 2 3 3 3 6 6 6 2 2 2 3 3\n",
      " 3 5 5 5 2 2 2 8 8 8 5 5 5 5 5 5 5 5 5 6 6 6 4 4 4 7 7 7 5 5 5 8 8 8 9 9 9\n",
      " 5 5 5 4 4 4 9 9 9 6 6 6 1 1 1 0 0 0 4 4 4 6 6 6 3 3 3 3 3 3 8 8 8 4 4 4 5\n",
      " 5 5 0 0 0 3 3 3 2 2 2 3 3 3 5 5 5 4 4 4 8 8 8 2 2 2 0 0 0 4 4 4 1 1 1 4 4\n",
      " 4 0 0 0 7 7 7 2 2 2 5 5 5 8 8 8 6 6 6 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 9 9 9\n",
      " 7 7 7 1 1 1 1 1 1 9 9 9 3 3 3 0 0 0 8 8 8 7 7 7 8 8 8 6 6 6 0 0 0 1 1 1 4\n",
      " 4 4 0 0 0 9 9 9 2 2 2 5 5 5 0 0 0 6 6 6 3 3 3 5 5 5 7 7 7 2 2 2 3 3 3 9 9\n",
      " 9 8 8 8 5 5 5 9 9 9 9 9 9 7 7 7 3 3 3 0 0 0 9 9 9 9 9 9 1 1 1 8 8 8 4 4 4\n",
      " 1 1 1 0 0 0 9 9 9 7 7 7 6 6 6 4 4 4 9 9 9 8 8 8 9 9 9 6 6 6 7 7 7 5 5 5 9\n",
      " 9 9 4 4 4]\n",
      "accuracy:  0.20892857142857144\n",
      "epoch:  7\n",
      "\tbatch:  0\n",
      "\t\tloss:  2179.2059765329695\n",
      "\tbatch:  1\n",
      "\t\tloss:  2308.6129782068374\n",
      "\tbatch:  2\n",
      "\t\tloss:  2143.9382761367933\n",
      "\tbatch:  3\n",
      "\t\tloss:  2096.5545008852996\n",
      "\tbatch:  4\n",
      "\t\tloss:  2352.444751660767\n",
      "\tbatch:  5\n",
      "\t\tloss:  2199.32718877564\n",
      "\tbatch:  6\n",
      "\t\tloss:  2025.719238031094\n",
      "\tbatch:  7\n",
      "\t\tloss:  1989.43352042753\n",
      "\tbatch:  8\n",
      "\t\tloss:  1912.688385858927\n",
      "\tbatch:  9\n",
      "\t\tloss:  2092.860593124389\n",
      "\tbatch:  10\n",
      "\t\tloss:  1991.2907744853173\n",
      "\tbatch:  11\n",
      "\t\tloss:  2023.6601931884884\n",
      "\tbatch:  12\n",
      "\t\tloss:  1887.6943596443398\n",
      "\tbatch:  13\n",
      "\t\tloss:  1782.5819461257256\n",
      "\tbatch:  14\n",
      "\t\tloss:  1844.3706594487005\n",
      "\tbatch:  15\n",
      "\t\tloss:  1984.4950487773772\n",
      "\tbatch:  16\n",
      "\t\tloss:  2279.5592420461053\n",
      "\tbatch:  17\n",
      "\t\tloss:  953.2505172688409\n",
      "y_pred:  [9 9 8 8 8 8 8 8 1 1 1 7 7 7 8 8 8 7 7 7 1 1 1 6 6 6 1 1 1 8 8 8 6 6 6 7 7\n",
      " 7 8 8 8 8 8 8 4 4 4 2 2 2 8 8 8 1 1 1 8 8 8 1 1 1 1 1 1 0 0 0 1 1 1 0 0 0\n",
      " 7 7 7 1 1 1 9 9 9 6 6 6 3 3 3 5 5 5 4 4 4 2 2 2 3 3 3 2 2 2 8 8 8 3 3 3 2\n",
      " 2 2 3 3 3 3 3 3 1 1 1 6 6 6 8 8 8 8 8 8 3 3 3 1 1 1 8 8 8 7 7 7 4 4 4 1 1\n",
      " 1 4 4 4 9 9 9 6 6 6 1 1 1 8 8 8 4 4 4 3 3 3 0 0 0 8 8 8 8 8 8 7 7 7 9 9 9\n",
      " 3 3 3 0 0 0 9 9 9 2 2 2 3 3 3 8 8 8 2 2 2 1 1 1 8 8 8 9 9 9 8 8 8 8 8 8 6\n",
      " 6 6 8 8 8 8 8 8 9 9 9 1 1 1 9 9 9 8 8 8 0 0 0 2 2 2 8 8 8 4 4 4 1 1 1 3 3\n",
      " 3 5 5 5 1 1 1 8 8 8 3 3 3 4 4 4 4 4 4 4 4 4 8 8 8 6 6 6 6 6 6 8 8 8 8 8 8\n",
      " 1 1 1 8 8 8 9 9 9 1 1 1 3 3 3 3 3 3 4 4 4 6 6 6 3 3 3 1 1 1 9 9 9 3 3 3 1\n",
      " 1 1 0 0 0 9 9 9 3 3 3 9 9 9 4 4 4 8 8 8 5 5 5 8 8 8 3 3 3 4 4 4 7 7 7 4 4\n",
      " 4 8 8 8 8 8 8 2 2 2 3 3 3 8 8 8 3 3 3 8 8 8 9 9 9 9 9 9 8 8 8 3 3 3 8 8 8\n",
      " 1 1 1 9 9 9 2 2 2 3 3 3 3 3 3 5 5 5 0 0 0 1 1 1 8 8 8 5 5 5 1 1 1 2 2 2 6\n",
      " 6 6 6 6 6 9 9 9 2 2 2 5 5 5 1 1 1 5 5 5 4 4 4 3 3 3 7 7 7 0 0 0 3 3 3 8 8\n",
      " 8 1 1 1 3 3 3 0 0 0 0 0 0 3 3 3 5 5 5 2 2 2 8 8 8 9 9 9 1 1 1 7 7 7 8 8 8\n",
      " 9 9 9 9 9 9 3 3 3 6 6 6 6 6 6 3 3 3 9 9 9 8 8 8 8 8 8 2 2 2 1 1 1 6 6 6 0\n",
      " 0 0 8 8 8]\n",
      "y:  [0 0 0 0 0 9 9 9 7 7 7 7 7 7 3 3 3 6 6 6 3 3 3 5 5 5 1 1 1 9 9 9 7 7 7 6 6\n",
      " 6 2 2 2 0 0 0 4 4 4 2 2 2 2 2 2 3 3 3 9 9 9 9 9 9 7 7 7 2 2 2 7 7 7 0 0 0\n",
      " 9 9 9 3 3 3 2 2 2 5 5 5 3 3 3 4 4 4 6 6 6 2 2 2 3 3 3 4 4 4 9 9 9 5 5 5 0\n",
      " 0 0 4 4 4 3 3 3 2 2 2 3 3 3 1 1 1 1 1 1 6 6 6 7 7 7 4 4 4 2 2 2 1 1 1 7 7\n",
      " 7 5 5 5 2 2 2 7 7 7 9 9 9 1 1 1 4 4 4 3 3 3 1 1 1 2 2 2 5 5 5 8 8 8 7 7 7\n",
      " 8 8 8 7 7 7 7 7 7 9 9 9 5 5 5 0 0 0 9 9 9 8 8 8 2 2 2 7 7 7 2 2 2 8 8 8 4\n",
      " 4 4 9 9 9 1 1 1 0 0 0 4 4 4 2 2 2 1 1 1 0 0 0 2 2 2 3 3 3 6 6 6 2 2 2 3 3\n",
      " 3 5 5 5 2 2 2 8 8 8 5 5 5 5 5 5 5 5 5 6 6 6 4 4 4 7 7 7 5 5 5 8 8 8 9 9 9\n",
      " 5 5 5 4 4 4 9 9 9 6 6 6 1 1 1 0 0 0 4 4 4 6 6 6 3 3 3 3 3 3 8 8 8 4 4 4 5\n",
      " 5 5 0 0 0 3 3 3 2 2 2 3 3 3 5 5 5 4 4 4 8 8 8 2 2 2 0 0 0 4 4 4 1 1 1 4 4\n",
      " 4 0 0 0 7 7 7 2 2 2 5 5 5 8 8 8 6 6 6 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 9 9 9\n",
      " 7 7 7 1 1 1 1 1 1 9 9 9 3 3 3 0 0 0 8 8 8 7 7 7 8 8 8 6 6 6 0 0 0 1 1 1 4\n",
      " 4 4 0 0 0 9 9 9 2 2 2 5 5 5 0 0 0 6 6 6 3 3 3 5 5 5 7 7 7 2 2 2 3 3 3 9 9\n",
      " 9 8 8 8 5 5 5 9 9 9 9 9 9 7 7 7 3 3 3 0 0 0 9 9 9 9 9 9 1 1 1 8 8 8 4 4 4\n",
      " 1 1 1 0 0 0 9 9 9 7 7 7 6 6 6 4 4 4 9 9 9 8 8 8 9 9 9 6 6 6 7 7 7 5 5 5 9\n",
      " 9 9 4 4 4]\n",
      "accuracy:  0.20892857142857144\n",
      "epoch:  8\n",
      "\tbatch:  0\n",
      "\t\tloss:  2123.007402785961\n",
      "\tbatch:  1\n",
      "\t\tloss:  2242.111204116907\n",
      "\tbatch:  2\n",
      "\t\tloss:  2120.1378979347364\n",
      "\tbatch:  3\n",
      "\t\tloss:  2079.3893905458353\n",
      "\tbatch:  4\n",
      "\t\tloss:  2334.440320169189\n",
      "\tbatch:  5\n",
      "\t\tloss:  2227.8238775526834\n",
      "\tbatch:  6\n",
      "\t\tloss:  1968.7091917229625\n",
      "\tbatch:  7\n",
      "\t\tloss:  1927.2698361683156\n",
      "\tbatch:  8\n",
      "\t\tloss:  1873.528251598411\n",
      "\tbatch:  9\n",
      "\t\tloss:  1968.7553558681448\n",
      "\tbatch:  10\n",
      "\t\tloss:  1839.6528141351773\n",
      "\tbatch:  11\n",
      "\t\tloss:  2104.228608427096\n",
      "\tbatch:  12\n",
      "\t\tloss:  1972.9974317862384\n",
      "\tbatch:  13\n",
      "\t\tloss:  1741.711722447696\n",
      "\tbatch:  14\n",
      "\t\tloss:  1725.6381463460752\n",
      "\tbatch:  15\n",
      "\t\tloss:  2093.049842276057\n",
      "\tbatch:  16\n",
      "\t\tloss:  2315.1115359796313\n",
      "\tbatch:  17\n",
      "\t\tloss:  937.0568257058242\n",
      "y_pred:  [9 9 8 8 8 1 1 1 2 2 2 7 7 7 8 8 8 3 3 3 1 1 1 6 6 6 1 1 1 8 8 8 6 6 6 6 6\n",
      " 6 8 8 8 3 3 3 4 4 4 2 2 2 2 2 2 3 3 3 0 0 0 1 1 1 1 1 1 3 3 3 1 1 1 0 0 0\n",
      " 9 9 9 1 1 1 9 9 9 6 6 6 3 3 3 5 5 5 4 4 4 2 2 2 3 3 3 2 2 2 9 9 9 3 3 3 2\n",
      " 2 2 3 3 3 3 3 3 1 1 1 0 0 0 3 3 3 8 8 8 3 3 3 1 1 1 8 8 8 7 7 7 4 4 4 1 1\n",
      " 1 4 4 4 9 9 9 6 6 6 1 1 1 9 9 9 4 4 4 3 3 3 3 3 3 8 8 8 3 3 3 7 7 7 1 1 1\n",
      " 3 3 3 0 0 0 9 9 9 2 2 2 3 3 3 8 8 8 2 2 2 1 1 1 8 8 8 9 9 9 8 8 8 2 2 2 5\n",
      " 5 5 0 0 0 1 1 1 9 9 9 0 0 0 9 9 9 8 8 8 0 0 0 2 2 2 8 8 8 4 4 4 3 3 3 3 3\n",
      " 3 5 5 5 1 1 1 8 8 8 3 3 3 4 4 4 6 6 6 4 4 4 8 8 8 6 6 6 6 6 6 8 8 8 0 0 0\n",
      " 4 4 4 5 5 5 9 9 9 1 1 1 3 3 3 3 3 3 4 4 4 6 6 6 3 3 3 1 1 1 9 9 9 3 3 3 5\n",
      " 5 5 0 0 0 9 9 9 3 3 3 4 4 4 4 4 4 8 8 8 5 5 5 2 2 2 3 3 3 4 4 4 7 7 7 6 6\n",
      " 6 2 2 2 8 8 8 2 2 2 3 3 3 8 8 8 3 3 3 8 8 8 9 9 9 9 9 9 8 8 8 3 3 3 8 8 8\n",
      " 1 1 1 9 9 9 2 2 2 3 3 3 3 3 3 5 5 5 0 0 0 1 1 1 8 8 8 5 5 5 1 1 1 2 2 2 6\n",
      " 6 6 6 6 6 9 9 9 2 2 2 5 5 5 1 1 1 5 5 5 4 4 4 3 3 3 7 7 7 0 0 0 3 3 3 8 8\n",
      " 8 4 4 4 3 3 3 0 0 0 0 0 0 3 3 3 5 5 5 7 7 7 8 8 8 9 9 9 1 1 1 7 7 7 3 3 3\n",
      " 9 9 9 9 9 9 3 3 3 3 3 3 6 6 6 3 3 3 9 9 9 8 8 8 3 3 3 5 5 5 1 1 1 6 6 6 0\n",
      " 0 0 3 3 3]\n",
      "y:  [0 0 0 0 0 9 9 9 7 7 7 7 7 7 3 3 3 6 6 6 3 3 3 5 5 5 1 1 1 9 9 9 7 7 7 6 6\n",
      " 6 2 2 2 0 0 0 4 4 4 2 2 2 2 2 2 3 3 3 9 9 9 9 9 9 7 7 7 2 2 2 7 7 7 0 0 0\n",
      " 9 9 9 3 3 3 2 2 2 5 5 5 3 3 3 4 4 4 6 6 6 2 2 2 3 3 3 4 4 4 9 9 9 5 5 5 0\n",
      " 0 0 4 4 4 3 3 3 2 2 2 3 3 3 1 1 1 1 1 1 6 6 6 7 7 7 4 4 4 2 2 2 1 1 1 7 7\n",
      " 7 5 5 5 2 2 2 7 7 7 9 9 9 1 1 1 4 4 4 3 3 3 1 1 1 2 2 2 5 5 5 8 8 8 7 7 7\n",
      " 8 8 8 7 7 7 7 7 7 9 9 9 5 5 5 0 0 0 9 9 9 8 8 8 2 2 2 7 7 7 2 2 2 8 8 8 4\n",
      " 4 4 9 9 9 1 1 1 0 0 0 4 4 4 2 2 2 1 1 1 0 0 0 2 2 2 3 3 3 6 6 6 2 2 2 3 3\n",
      " 3 5 5 5 2 2 2 8 8 8 5 5 5 5 5 5 5 5 5 6 6 6 4 4 4 7 7 7 5 5 5 8 8 8 9 9 9\n",
      " 5 5 5 4 4 4 9 9 9 6 6 6 1 1 1 0 0 0 4 4 4 6 6 6 3 3 3 3 3 3 8 8 8 4 4 4 5\n",
      " 5 5 0 0 0 3 3 3 2 2 2 3 3 3 5 5 5 4 4 4 8 8 8 2 2 2 0 0 0 4 4 4 1 1 1 4 4\n",
      " 4 0 0 0 7 7 7 2 2 2 5 5 5 8 8 8 6 6 6 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 9 9 9\n",
      " 7 7 7 1 1 1 1 1 1 9 9 9 3 3 3 0 0 0 8 8 8 7 7 7 8 8 8 6 6 6 0 0 0 1 1 1 4\n",
      " 4 4 0 0 0 9 9 9 2 2 2 5 5 5 0 0 0 6 6 6 3 3 3 5 5 5 7 7 7 2 2 2 3 3 3 9 9\n",
      " 9 8 8 8 5 5 5 9 9 9 9 9 9 7 7 7 3 3 3 0 0 0 9 9 9 9 9 9 1 1 1 8 8 8 4 4 4\n",
      " 1 1 1 0 0 0 9 9 9 7 7 7 6 6 6 4 4 4 9 9 9 8 8 8 9 9 9 6 6 6 7 7 7 5 5 5 9\n",
      " 9 9 4 4 4]\n",
      "accuracy:  0.24107142857142858\n",
      "epoch:  9\n",
      "\tbatch:  0\n",
      "\t\tloss:  2206.5286110577517\n",
      "\tbatch:  1\n",
      "\t\tloss:  2205.222804626903\n",
      "\tbatch:  2\n",
      "\t\tloss:  2138.2503690003145\n",
      "\tbatch:  3\n",
      "\t\tloss:  2094.4460664371554\n",
      "\tbatch:  4\n",
      "\t\tloss:  2317.784563043395\n",
      "\tbatch:  5\n",
      "\t\tloss:  2258.967627634567\n",
      "\tbatch:  6\n",
      "\t\tloss:  1678.584658047148\n",
      "\tbatch:  7\n",
      "\t\tloss:  1989.4335203148553\n",
      "\tbatch:  8\n",
      "\t\tloss:  1836.787396579599\n",
      "\tbatch:  9\n",
      "\t\tloss:  1880.4654491732945\n",
      "\tbatch:  10\n",
      "\t\tloss:  1731.4312228655112\n",
      "\tbatch:  11\n",
      "\t\tloss:  1998.1741165676399\n",
      "\tbatch:  12\n",
      "\t\tloss:  1823.647586319652\n",
      "\tbatch:  13\n",
      "\t\tloss:  1856.010047684402\n",
      "\tbatch:  14\n",
      "\t\tloss:  1904.7744375450932\n",
      "\tbatch:  15\n",
      "\t\tloss:  2030.875335378626\n",
      "\tbatch:  16\n",
      "\t\tloss:  2093.0504045115053\n",
      "\tbatch:  17\n",
      "\t\tloss:  910.7904976624832\n",
      "y_pred:  [9 9 8 8 8 8 8 8 1 1 1 7 7 7 8 8 8 7 7 7 1 1 1 5 5 5 1 1 1 8 8 8 6 6 6 7 7\n",
      " 7 2 2 2 3 3 3 4 4 4 2 2 2 2 2 2 1 1 1 8 8 8 1 1 1 1 1 1 3 3 3 1 1 1 0 0 0\n",
      " 7 7 7 1 1 1 9 9 9 6 6 6 3 3 3 5 5 5 4 4 4 2 2 2 3 3 3 2 2 2 9 9 9 3 3 3 2\n",
      " 2 2 3 3 3 3 3 3 1 1 1 0 0 0 3 3 3 8 8 8 3 3 3 1 1 1 8 8 8 7 7 7 4 4 4 1 1\n",
      " 1 4 4 4 9 9 9 7 7 7 1 1 1 9 9 9 4 4 4 3 3 3 0 0 0 8 8 8 3 3 3 9 9 9 1 1 1\n",
      " 3 3 3 0 0 0 1 1 1 2 2 2 3 3 3 8 8 8 2 2 2 1 1 1 8 8 8 9 9 9 8 8 8 2 2 2 5\n",
      " 5 5 8 8 8 9 9 9 9 9 9 0 0 0 9 9 9 8 8 8 0 0 0 2 2 2 8 8 8 4 4 4 1 1 1 3 3\n",
      " 3 5 5 5 1 1 1 8 8 8 3 3 3 4 4 4 6 6 6 4 4 4 5 5 5 6 6 6 6 6 6 8 8 8 0 0 0\n",
      " 4 4 4 5 5 5 7 7 7 4 4 4 3 3 3 3 3 3 5 5 5 6 6 6 3 3 3 1 1 1 9 9 9 3 3 3 1\n",
      " 1 1 0 0 0 9 9 9 3 3 3 9 9 9 4 4 4 8 8 8 5 5 5 8 8 8 3 3 3 4 4 4 7 7 7 4 4\n",
      " 4 2 2 2 8 8 8 2 2 2 3 3 3 8 8 8 3 3 3 8 8 8 9 9 9 9 9 9 8 8 8 3 3 3 8 8 8\n",
      " 1 1 1 9 9 9 2 2 2 3 3 3 3 3 3 5 5 5 0 0 0 1 1 1 8 8 8 5 5 5 1 1 1 3 3 3 6\n",
      " 6 6 6 6 6 9 9 9 2 2 2 5 5 5 1 1 1 5 5 5 4 4 4 3 3 3 7 7 7 0 0 0 3 3 3 8 8\n",
      " 8 1 1 1 3 3 3 0 0 0 0 0 0 3 3 3 5 5 5 7 7 7 8 8 8 9 9 9 1 1 1 7 7 7 8 8 8\n",
      " 2 2 2 9 9 9 3 3 3 3 3 3 1 1 1 3 3 3 9 9 9 8 8 8 8 8 8 2 2 2 1 1 1 5 5 5 0\n",
      " 0 0 3 3 3]\n",
      "y:  [0 0 0 0 0 9 9 9 7 7 7 7 7 7 3 3 3 6 6 6 3 3 3 5 5 5 1 1 1 9 9 9 7 7 7 6 6\n",
      " 6 2 2 2 0 0 0 4 4 4 2 2 2 2 2 2 3 3 3 9 9 9 9 9 9 7 7 7 2 2 2 7 7 7 0 0 0\n",
      " 9 9 9 3 3 3 2 2 2 5 5 5 3 3 3 4 4 4 6 6 6 2 2 2 3 3 3 4 4 4 9 9 9 5 5 5 0\n",
      " 0 0 4 4 4 3 3 3 2 2 2 3 3 3 1 1 1 1 1 1 6 6 6 7 7 7 4 4 4 2 2 2 1 1 1 7 7\n",
      " 7 5 5 5 2 2 2 7 7 7 9 9 9 1 1 1 4 4 4 3 3 3 1 1 1 2 2 2 5 5 5 8 8 8 7 7 7\n",
      " 8 8 8 7 7 7 7 7 7 9 9 9 5 5 5 0 0 0 9 9 9 8 8 8 2 2 2 7 7 7 2 2 2 8 8 8 4\n",
      " 4 4 9 9 9 1 1 1 0 0 0 4 4 4 2 2 2 1 1 1 0 0 0 2 2 2 3 3 3 6 6 6 2 2 2 3 3\n",
      " 3 5 5 5 2 2 2 8 8 8 5 5 5 5 5 5 5 5 5 6 6 6 4 4 4 7 7 7 5 5 5 8 8 8 9 9 9\n",
      " 5 5 5 4 4 4 9 9 9 6 6 6 1 1 1 0 0 0 4 4 4 6 6 6 3 3 3 3 3 3 8 8 8 4 4 4 5\n",
      " 5 5 0 0 0 3 3 3 2 2 2 3 3 3 5 5 5 4 4 4 8 8 8 2 2 2 0 0 0 4 4 4 1 1 1 4 4\n",
      " 4 0 0 0 7 7 7 2 2 2 5 5 5 8 8 8 6 6 6 1 1 1 8 8 8 1 1 1 1 1 1 6 6 6 9 9 9\n",
      " 7 7 7 1 1 1 1 1 1 9 9 9 3 3 3 0 0 0 8 8 8 7 7 7 8 8 8 6 6 6 0 0 0 1 1 1 4\n",
      " 4 4 0 0 0 9 9 9 2 2 2 5 5 5 0 0 0 6 6 6 3 3 3 5 5 5 7 7 7 2 2 2 3 3 3 9 9\n",
      " 9 8 8 8 5 5 5 9 9 9 9 9 9 7 7 7 3 3 3 0 0 0 9 9 9 9 9 9 1 1 1 8 8 8 4 4 4\n",
      " 1 1 1 0 0 0 9 9 9 7 7 7 6 6 6 4 4 4 9 9 9 8 8 8 9 9 9 6 6 6 7 7 7 5 5 5 9\n",
      " 9 9 4 4 4]\n",
      "accuracy:  0.21964285714285714\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model.train(X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val, learning_rate=0.0000008, epochs=10, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b33069848e313ad49d053c5e2afad7bf1a5908034c1949d44822ff536d0b5b4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
